# 测量内容为最大负载带宽和时延
基础设计：
1. CPU0负责把报文分到CPU1，CPU2上,同时进行信息更新等其他操作
2. CPU1，CPU2分别进行报文处理，中间经过一个令牌请求操作

？是否需要使用不同协议进行测量？——自动化测试的时候可以进行

## main函数保持不变，去除令牌请求;即：直接尽力转发

发送一种数据包(所有流量在一个核内)：
	64B小包，发送13.7Mpps，没有出现包无法入队的情况，接收流量为13.6Mpps
	100B包，打开时延测量，大致248us【DPDK有问题，虽然报文基本全部收到，但是测试的接收到的pps却是发送的一半】
	
发送多种数据包(流量均分到两个核):
	64B小包，发送13.8Mpps，接收12.3Mpps
	——结合上面测试，需要分析是否是测试环境问题————调小速率也会有一定的丢包
	——结合corruption的知识，是否可能是网络连接或者光纤有问题

## 直接使用加锁的多核HTB；每个flow都需要维护树高的令牌桶

### 环境： 所有流量都直接在底层完成
flow1   flow2分离

发送一种数据包(所有流量在一个核内)：
	64B小包，发送14.5Mpps，接收7.5Mpps,出现报文无法入队的情况————【增加对于队列长度的比较？加大队列长度应该可以提高吞吐率；或者改变每次读取burst的大小】
	100B包，打开时延测量，大致503us
	
发送多种数据包(流量均分到两个核):
	64B小包，发送10.0Mpps，接收接近10.0Mpps，没有无法入队的情况

### 环境：存在父节点，需要维护父节点带宽(有锁的竞争)
   parent
   __|__
  |     | 
flow1 flow2

发送一种数据包
	64B, 发送10.3Mpps,接收5.5Mpps，出现报文无法入队
	100B，发送7.8Mpps,接收3.9Mpps，时延: 615us
发送两种数据包
	64B, 发送14.5Mpps,接收7.4Mpps,出现报文无法入队
	
### 环境：存在复杂的父节点共享情况
		root
	 p1		  p2
   c1  c2 = c1  c2
发送一种
	64B，发送14.7Mpss,接收4.2Mpps，出现报文无法入队
	100B，发送7.2Mpps,接收3.6Mpps,时延：732us
发送多种
	64B，发送14.6Mpps,接收5.9Mpps

### 环境： 更加复杂的父节点共享带宽
	root
 32个父节点
1024个子节点

发送多种
	64B，发送14.6Mpps，接收5.8Mpps--因为锁发生在Core之间，所以和上面情况一致【强调锁的竞争应该加高树高】

### 对于流量限制的测试
单个flow运行情况下，限速没有问题
构造有共享带宽后情形，
root(100Kpps), parent(40,80Kpps), leaf(20,60Kpps)
结果整机有10%的偏差
——可能是令牌数目引起的
——调整令牌数目，结果仍然如此
——猜测就是由于CPU的timestamp引起的？？

## 父节点的令牌更新由一个核单独完成——仍然有锁，但是锁被减少



## 两种方案结合

## 直接使用Coral加更新的多核HTB

### 环境： 所有流量都直接在底层完成
flow1   flow2分离

发送一种数据包(所有流量在一个核内)：
	64B小包，发送13.9Mpps，接收12.3Mpps
	100B包，打开时延测量，大致271us
	
发送多种数据包(流量均分到两个核):
	64B小包，发送10.0Mpps，接收接近10.0Mpps，没有无法入队的情况

### 环境：存在父节点，查看限速结果
root(100Kpps), parent(40,80Kpps), leaf(20,60Kpps)
1. 一条流的上限能被限制
2. 一条流的在没达到上限的情况下，分配给的流量基本和需求一致；但是有一定偏差
——需要更详细测试，以及能否快速响应变化的测试
——或者增加设计，处理突发流量
3. 多条流的情况下,稳定限速能够满足要求；但是快速变化的测试不能看出效果

### 环境： 更加复杂的父节点共享带宽
	root
 32个父节点
1024个子节点
最恶劣的情况，正常更新--经过测试，大约0.1ms内可以完成全量更新
——最坏情况，1ms内存在大量的突发流量～流量并不平稳

如果250ms更新一次的话,

